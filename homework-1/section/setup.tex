\section{Experimental Setup}
\label{sec:setup}

Describe the experimental setup, i.e.

\subsection{Collections}

	The collections used throughout the process of system development were the ones provided by CLEF for the Touché 2022 edition. Those include:

	\begin{enumerate}
		\item topics-task2.xml which contains the topics.
		\item The original version of passages.jsonl which contains the documents.
		\item DocT5Query expanded version of passages.jsonl which contains the documents expanded with queries generated using DocT5Query.
	\end{enumerate}

\subsection{Evaluation measures}

	The evaluation measure used is Normalized Discounted Cumulative Gain at depth 5, NDCG@5 in short.

	It is the evaluation measure used by Touché to officially evaluate runs.

	NDCG@k is calculated as follows:

	$$
	NDCG@k = \frac{DCG@k}{iDCG@k}
	$$
	where
	$$
	DCG@k = \sum_{i=1}^{k}\frac{relevance_i}{log_2(i+1)}
	$$
	and iDCG@k is the ideal DCG@k, meaning the DCG@k for documents ordered by relevance, highest to lowest.


\subsection{Git repository}

	The project’s development can be found in the following link to its Git \href{https://bitbucket.org/upd-dei-stud-prj/seupd2122-kueri/src/master/}{repository}.
	

\subsection{Hardware}

	The specifications of the computer used to perform the runs are the following:
	\begin{description}
		\item[OS] Windows 10 Home 21H2 x64
		\item[CPU] AMD Ryzen 5 1600 @ 3.9GHz
		\item[RAM] 16GB 3000mhz cl16
		\item[GPU] Nvidia GTX 1060 6GB
		\item[HDD] 2TB 7200RPM
	\end{description}