\section{Methodology}
\label{sec:methodology}


The following is the class diagram for our implementation: Figure \ref{fig:class-diagram}

\begin{figure}[h]
	\centering
	\includegraphics[width=\textwidth]{figure/class-diagram.pdf}
	\caption{Class diagram of the project}
	\label{fig:class-diagram}
\end{figure}

The developed Java system is divided into the following packages, each package representing a stage: Parse, Analyze, Index, Filter, Search, RF, RRF, and Argument Quality.

\subsection{Parse, Analyze, Index}
\label{subsec:parse, analyze, index}
  
  These packages are in charge of the database creation and to prepare topics.
  
  The documents in the DocT5Query expanded corpus are parsed, their text field analyzed (with the possibility of using different custom analyzers) and then indexed with the fields: ID, Body and DocT5Query.
  
  The topics are also parsed so that the fields number, title and objects can be used in the search using Lucene, with the latter two also analyzed.
 
\subsection{Filter}

This class allows to extract the strings from the object field in the topics and return a BooleanQuery.Builder object, which can be later consumed by the search method by adding it as a MUST clause in the search, to only retrieve documents that present all terms contained in the object fields.

\subsection{Search}
\label{subsec:search}
  
  This packed is responsible responsible for:
    \begin{enumerate}
    	\item Calling the Paese and Analyze packages to retrieve and preparing the topics for the search.
    	\item Defining which type of comparison to perform between topics and documents, that can be chosen by changing the similarity function.
    	\item Defining how to use topics in the search.
    	
    	The topics titles are used search by similarity with a SHOULD clause, it possible to also assign weights to the different fields of the documents among which to search, or to select just one of the two fields (Contents and DocT5Query), and the MUST clause described in the Filter class can be added.
    	\item Writing the results on a file
    \end{enumerate}

\subsection{RF}
        
        RF is a customized class with the goal of performing a search using explicit relevance feedback to perform query expansion.
        
        RF functions in a similar way to the Searcher class, with the exception of building the query used in the searching using the tokens present in relevant documents, instead of using the terms in title field of the topics file.
        
        The class collects all docID and relevance of relevant documents in the \textit{qrels} file.
        
        The tokens and their frequency in the relevant documents are retrieved by searching the document by docID and iterating through its termvector.
        
        The tokens used in the search are boosted by their frequency in the document multiplied by the square of the relevance score.
        
        Relevance Feedback is standardly based on the Rocchio Algorithm.
        The formula for the Rocchio Algorithm is:
        $$
        \overrightarrow{Q_{m}}=
        \left(a\cdot\overrightarrow{Q_{O}}\right)+
        \left(b\cdot\frac{1}{|D_{r}|}\cdot\sum_{\overrightarrow{D_{j}}\in D_{r}}\overrightarrow{D_{j}}\right)-
        \left(c\cdot\frac{1}{|D_{nr}|}\cdot\sum_{\overrightarrow{D_{k}}\in D_{nr}}\overrightarrow{D_{k}}\right)
        $$
        where $\overrightarrow{Q_{m}}$ is the modified query vector, $\overrightarrow{Q_{O}}$ is the original query vector, $\overrightarrow{D_{i}}$ is the document vector for the $i^{th}$ document, $D_{r}$ is the set of relevant documents, $D_{nr}$ is the set of non-relevant documents and $a$, $b$ and $c$ are weight parameters.
        
        In our case the parameters used are 0, 1, 0.
        
        Rocchio algorithm is however defined for working with binary relevance, since this collection uses multi-graded relevance, our version of RF is customized to take into account the different relevance scores used (0 to 3).
        
        The custom formula we used is:
        $$
        \overrightarrow{Q_{m}}=
        k_i^2\cdot\frac{1}{|D_{r}|}\cdot\sum_{\overrightarrow{D_{i}}\in D_{r}}\overrightarrow{D_{i}}
        $$
        where $\overrightarrow{Q_{m}}$ is the modified query vector, $\overrightarrow{D_{i}}$ is the document vector for the $i^{th}$ document, $D_{r}$ is the set of relevant documents, and $k_i$ is the relevance score of the $i^{th}$ document.
         
         In this work a total of 491 relevant documents have been used to perform Relevance Feedback.
         
        The results of the search are then outputted as a standard run file.
        
\subsection{RRF}
  
      This package contains a single class, also called RRF.
        
        RRF.java is a customized class with the goal of performing using Reciprocal Ranking Fusion \citep{RRF} to fuse the results of different runs in a single one.
        
        RRF takes in imput a directory path and performs RRF using all the runs in .txt documents inside that directory.
        
        For each documents and for each topic the documents and their respective ranking are collected.
        
        Then document receive a new scoring using the RRF formula.
        
        Given a set of documents \textit{D} and a set of rankings \textit{R} for the documents, the formula for RRF is:
        $$RRFscore(d \in D)=\sum_{r \in R}^{}\frac{1}{k+r(d)}$$
        where k is a fixed number, in this case k is set to 30.
        
        Then, for each topic, documents are ranked (and ordered) based on their RRF score.
        
        The results of the search are then outputted as a standard run file.

\subsection{Argument quality}
  \label{subsec:Argument quality}
  We decided to make use of IBM Project Debater API.
  
      Project Debater is an AI system used to perform various tasks about debating at a human level. IBM makes freely available, for research purposes, some services based on this system through an API. \citep{ProjectDebaterAPI}
      
      We were interested in the argument quality service of the API. It accepts a couple of strings labeled as Sentence and Topic, and it returns a float score in the range 0-1 based on the relevance of the sentence for the topic and on the quality of the sentence as a text, which means how good it is written.
      
      Since the rest of our system is designed to already score documents based on the relevance to the topic, we now just wanted to evaluate the text quality. In order to do so, for each document in the collection we decided to send Sentence-Topic pairs in which the Sentence was the body of the document and the Topic was an empty string.
      
      We coded the ArgumentQualityVerifier class which evaluates the written quality of each document by using the API and then saves the scores to a file.
      
      Then we had to use the obtained scores to rerank the results of the search saved in a run file. So we defined the ArgumentQualityReranker class which:
      
       \begin{enumerate} 
           \item loads the quality scores of all the documents from the file into a Map object \item iterates over the lines of the old run file and for each: multiplies the old score by the one assigned by Project Debater API and saves the object representing the new line to a list \item sorts the list of new lines by topic number and score and writes them on a new run file 
       \end{enumerate}